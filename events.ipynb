{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ligo.skymap.io import read_sky_map\n",
    "import astropy_healpix as ah\n",
    "import numpy as np\n",
    "from ligo.gracedb.rest import GraceDb\n",
    "import wget\n",
    "from astropy.io import fits\n",
    "import requests\n",
    "import os\n",
    "import healpy as hp\n",
    "from astropy.coordinates import SkyCoord\n",
    "from ligo.skymap.io import read_sky_map\n",
    "from ligo.skymap.postprocess import crossmatch\n",
    "from astropy import units as u\n",
    "import astropy.cosmology.units as cu\n",
    "from astropy.cosmology import WMAP9\n",
    "from astropy.table import Table, vstack,unique\n",
    "from astropy import table\n",
    "from astropy.io import ascii\n",
    "import pandas as pd\n",
    "import mastcasjobs\n",
    "import time\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def area_90(graceid):\n",
    "    # input a list of GraceIDs and return the area of the 90% probability region and other information for each event and the mean,std of the distance\n",
    "\n",
    "    client = GraceDb()\n",
    "    areas = np.zeros(len(graceid))\n",
    "    distmean = np.zeros(len(graceid))\n",
    "    diststd = np.zeros(len(graceid))\n",
    "    ras = np.zeros(len(graceid))\n",
    "    decs = np.zeros(len(graceid))\n",
    "    prob_dec_gt_m30 = np.zeros(len(graceid))\n",
    "\n",
    "    for j in range(len(graceid)):\n",
    "        name = graceid[j]\n",
    "\n",
    "        if os.path.exists('/data/GW_events/data/{}_multiorder.fits'.format(name)):\n",
    "            # If the skymap file already exists, read it directly\n",
    "            skymap = read_sky_map('/data/GW_events/data/{}_multiorder.fits'.format(name), distances=True, nest=True, moc=True)\n",
    "\n",
    "        else: \n",
    "            #Get the event skymap\n",
    "            response = client.superevent(name)\n",
    "            file = response.json()['links']['files']\n",
    "\n",
    "            suffixes = ['Bilby.multiorder.fits','Bilby.offline0.multiorder.fits','bayestar.multiorder.fits']\n",
    "            url_found = False\n",
    "            \n",
    "            for suffix in suffixes:\n",
    "                url = f\"{file}{suffix}\"\n",
    "                try:\n",
    "                    print(url)\n",
    "                    response = requests.get(url)\n",
    "                    if response.status_code == 200:\n",
    "                        url_found = True\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    print(f'Error fetching URL {url}: {e}')\n",
    "\n",
    "            if not url_found:\n",
    "                print(f'No skymap found for event {name} at index {j}')\n",
    "                exit(0) # 跳过当前事件，继续处理下一个\n",
    "\n",
    "            print(j, name, url)\n",
    "            print(wget.download(url, out = '/data/GW_events/data/{}_multiorder.fits'.format(name)))\n",
    "            skymap = read_sky_map('/data/GW_events/data/{}_multiorder.fits'.format(name), distances=True, nest=True, moc=True)\n",
    "            \n",
    "\n",
    "\n",
    "        # Find the 90% Probability Region\n",
    "        skymap.sort('PROBDENSITY', reverse=True)\n",
    "        level,ipix = ah.uniq_to_level_ipix(skymap['UNIQ'])\n",
    "        nside = ah.level_to_nside(level)\n",
    "        pixel_area = ah.nside_to_pixel_area(nside)\n",
    "        prob = pixel_area * skymap['PROBDENSITY']\n",
    "        cumprob = np.cumsum(prob)\n",
    "        i = cumprob.searchsorted(0.9)\n",
    "        area_90 = pixel_area[:i].sum()\n",
    "        areas[j] = '{:.4f}'.format(area_90.to_value(u.deg**2))\n",
    "        print(area_90.to_value(u.deg**2))\n",
    "\n",
    "        # Find the most probable ra and dec\n",
    "        i = np.argmax(skymap['PROBDENSITY'])\n",
    "        uniq = skymap[i]['UNIQ']\n",
    "        level, ipix = ah.uniq_to_level_ipix(uniq)\n",
    "        nside = ah.level_to_nside(level)\n",
    "        ra, dec = ah.healpix_to_lonlat(ipix, nside, order='nested')\n",
    "        ras[j] = '{:.4f}'.format(ra.deg)\n",
    "        decs[j] = '{:.4f}'.format(dec.deg)\n",
    "\n",
    "        # cauluclate the probability of dec > -30\n",
    "        level,ipix = ah.uniq_to_level_ipix(skymap['UNIQ'])\n",
    "        nside = ah.level_to_nside(level)\n",
    "        pixel_area = ah.nside_to_pixel_area(nside)\n",
    "        prob = pixel_area * skymap['PROBDENSITY']\n",
    "        lon = np.zeros(len(nside))\n",
    "        lat = np.zeros(len(nside))\n",
    "        for k in range(len(nside)):\n",
    "            lon[k], lat[k] = hp.pix2ang(nside[k], ipix[k], nest=True, lonlat=True) \n",
    "        index = np.where(lat > -30)\n",
    "        prob_dec_gt_m30[j] = '{:.4f}'.format(prob[index].sum())\n",
    "\n",
    "\n",
    "        # get the mean and std of the distance\n",
    "        hdul = fits.open(\"/data/GW_events/data/{}_multiorder.fits\".format(name))\n",
    "        hdr = hdul[1].header\n",
    "        distmean[j] = '{:.4f}'.format(hdr['DISTMEAN'])\n",
    "        diststd[j] = '{:.4f}'.format(hdr['DISTSTD'])\n",
    "        hdul.close()\n",
    "\n",
    "    return areas, distmean, diststd, ras, decs, prob_dec_gt_m30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossmatch_mq(gracedbid, out_file):\n",
    "\n",
    "    #get the milliquas catalogue\n",
    "    # if don't have the milliquas catalogue, you can download it from Vizier as follows:\n",
    "    # from astroquery.vizier import VizierClass\n",
    "    # vizier = VizierClass(\n",
    "    # row_limit=-1,\n",
    "    # columns=['recno', 'RAJ2000', 'DEJ2000', 'z','Name','Type']\n",
    "    # )\n",
    "    # cat, = vizier.get_catalogs('VII/294/catalog')\n",
    "    # cat.sort('recno')  # sort catalog so that doctest output is stable\n",
    "    # del cat['recno']\n",
    "    cat = ascii.read('/data/milliquas.csv', format='csv')\n",
    "    # calculate the distance of each milliquas from z\n",
    "    z = cat['z'] * cu.redshift\n",
    "    dis = z.to(u.Mpc, cu.redshift_distance(WMAP9, kind=\"luminosity\"))\n",
    "    cat.add_column(dis,name='Distance')\n",
    "    coordinates = SkyCoord(cat['RAJ2000']*u.deg, cat['DEJ2000']*u.deg,cat['Distance'])\n",
    "    res = Table()   #the matched catalogue of all events\n",
    "    match_num = np.zeros(len(gracedbid),dtype=int)  #the number of matched AGNs for each event\n",
    "\n",
    "    for j in range(len(gracedbid)):\n",
    "        #read skymap\n",
    "        name = gracedbid[j]\n",
    "        \n",
    "        if os.path.exists('/data/GW_events/match_mq/{}.csv'.format(name)):\n",
    "            match_tab = ascii.read('/data/GW_events/match_mq/{}.csv'.format(name), format='csv')\n",
    "            match_num[j] = len(match_tab)\n",
    "            if match_num[j] == 0:\n",
    "                continue\n",
    "        else:\n",
    "            skymap = read_sky_map('/data/GW_events/data/{}_multiorder.fits'.format(name), distances=True, nest=True, moc=True)\n",
    "            result = crossmatch(skymap, coordinates)\n",
    "            match_tab = cat[result.searched_prob_vol < 0.9]  #the matched catalogue of each event\n",
    "            match_tab.write('/data/GW_events/match_mq/{}.csv'.format(name), format='csv', overwrite=True)\n",
    "            match_num[j] = len(match_tab)\n",
    "        if j==0:\n",
    "            res = match_tab\n",
    "        else:\n",
    "            res = vstack([res, match_tab], join_type='exact')\n",
    "             \n",
    "    #save the catlogue\n",
    "    res = table.unique(res, keys='Name')\n",
    "    res.write('/data/GW_events/{}.csv'.format(out_file), format='csv', overwrite=True)\n",
    "\n",
    "    return match_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the events csv\n",
    "event = 'O4b_events'\n",
    "area = '500deg'\n",
    "O4ab = event[0:3]\n",
    "file_path = \"\"  #the path of the GW events csv file\n",
    "events = ascii.read(file_path,format='csv')\n",
    "graceid = events['superevent_id']\n",
    "print(graceid)\n",
    "areas, distmean, diststd, ra, dec, prob_dec_gt_m30 = area_90(graceid)\n",
    "match_num = crossmatch_mq(graceid, out_file='/{}/{}/{}_withoutfilter_mq_all'.format(area, O4ab , event))\n",
    "events.add_column(areas, name='area_90')\n",
    "events.add_column(distmean, name='distmean')\n",
    "events.add_column(diststd, name='diststd')\n",
    "events.add_column(ra, name='ra')\n",
    "events.add_column(dec, name='dec')\n",
    "events.add_column(prob_dec_gt_m30, name='prob_dec_gt_m30')\n",
    "events.add_column(match_num, name='match_num')\n",
    "d = events['distmean'] * u.Mpc\n",
    "z = d.to(cu.redshift, cu.redshift_distance(WMAP9, kind=\"comoving\", zmax=1200))\n",
    "events.add_column(z, name='zmean')\n",
    "\n",
    "# save the events csv file without filter, with more detailed information\n",
    "events.write('/data/GW_events/{}/{}/{}_without_filter.csv'.format(area,O4ab, event), format='csv', overwrite=True)\n",
    "index = np.where(areas < float(area[0:3]))\n",
    "events = events[index]\n",
    "print(events)\n",
    "print(len(events))\n",
    "events.write('/data/GW_events/{}/{}/{}_filtered_{}.csv'.format(area,O4ab, event, area), format='csv', overwrite=True)\n",
    "\n",
    "#crossmatch with mq catalog\n",
    "if len(index)>0:\n",
    "    crossmatch_mq(events['superevent_id'], out_file='/{}/{}/{}_{}_mq_matched'.format(area, O4ab, event,area))\n",
    "else:\n",
    "    print(f'No events have area_90 < {area}^2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter with declination > -30\n",
    "mq = pd.read_csv(f'/data/GW_events/{area}/{O4ab}/{event}_{area}_mq_matched.csv')\n",
    "print(\"All match AGNs:\", len(mq))\n",
    "dec_gtm30 = mq[mq['DEJ2000'] > -30]\n",
    "dec_gtm30.to_csv(f'/data/GW_events/{area}/{O4ab}/{event}_{area}_mq_matched_dec_gtm30.csv', index=False)\n",
    "print(\"Matched AGNs with declination > -30:\", len(dec_gtm30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use MAST CasJobs to filter events by PS1 g/r mag\n",
    "from astropy.table import join\n",
    "# MAST CasJobs credentials\n",
    "user = \"supernova\"\n",
    "pwd = \"sbs1rqlb6x\"\n",
    "\n",
    "jobs = mastcasjobs.MastCasJobs(username=user, password=pwd, context=\"PanSTARRS_DR2\",request_type='POST')\n",
    "\n",
    "# get the table of events\n",
    "events = ascii.read(f'/data/GW_events/{area}/{O4ab}/{event}_{area}_mq_matched.csv',format='csv')\n",
    "events_withoutname = events.copy()\n",
    "events_withoutname.remove_column('Name')\n",
    "jobs.drop_table_if_exists(f'{event}_{area}_mq_matched_PS1')\n",
    "jobs.drop_table_if_exists(f'{event}_{area}_mq_matched')\n",
    "jobs.upload_table(f'{event}_{area}_mq_matched', events_withoutname, verbose=True)\n",
    "\n",
    "\n",
    "query=f\"\"\"SELECT d.RAJ2000, d.DEJ2000, d.z, d.Type, d.Distance,\n",
    "o.objID, o.gMeanKronMag, o.rMeanKronMag,o.nDetections,\n",
    "soa.primaryDetection\n",
    " INTO mydb.[{event}_{area}_mq_matched_PS1]\n",
    " FROM mydb.[{event}_{area}_mq_matched] d\n",
    "CROSS APPLY dbo.fGetNearbyObjEq(d.RAJ2000, d.DEJ2000, 3.0/60.0) as x\n",
    "JOIN MeanObjectView o on o.ObjID=x.ObjId\n",
    "LEFT JOIN StackObjectAttributes AS soa ON soa.objID = x.objID\n",
    "WHERE o.nDetections>5\n",
    "AND soa.primaryDetection>0\n",
    "AND o.gMeanKronMag < 21\n",
    "AND o.rMeanKronMag < 21\n",
    "AND o.gMeanKronMag > 0\n",
    "AND o.rMeanKronMag > 0\n",
    "AND o.decMean > -30\n",
    "\"\"\"\n",
    "print(query)\n",
    "jobid = jobs.submit(query, task_name='filter with g/r mag')\n",
    "print(jobid)\n",
    "code,status = jobs.status(jobid)   #status: ready, started, finished, failed;code:0,1,5\n",
    "while status not in ['finished', 'failed']:\n",
    "    print(code, status)\n",
    "    time.sleep(5)  # 等待10秒后再次检查状态\n",
    "    code,status = jobs.status(jobid)\n",
    "if status == 'failed':\n",
    "    raise Exception(\"Job failed to complete.\")\n",
    "\n",
    "events_filtered = jobs.fast_table(f'{event}_{area}_mq_matched_PS1', verbose=True)\n",
    "print(len(events_filtered))\n",
    "\n",
    "events_filtered = join(events_filtered,events,join_type='left',keys=['z','Type','Distance'])\n",
    "events_filtered.remove_columns(['RAJ2000_1',\"DEJ2000_1\"])\n",
    "events_filtered.rename_column('RAJ2000_2','RAJ2000')\n",
    "events_filtered.rename_column('DEJ2000_2','DEJ2000')\n",
    "\n",
    "events_filtered.write(f'/data/GW_events/{area}/{O4ab}/{event}_{area}_mq_matched_PS1.csv', format='csv', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PS1_API_crossmatch(ra:np.array, dec:np.array, uploadCSV_path:str=\"./\"):\n",
    "    match_data = pd.DataFrame({\"ra\":ra, \"dec\":dec})\n",
    "    match_length = len(ra)\n",
    "    single_match_num = 4900\n",
    "    split_idx = list(range(0,match_length,single_match_num))\n",
    "    split_idx.append(match_length)\n",
    "    \n",
    "    # 使用API做crossmatch匹配半径只能为3角秒, When using API for crossmatch, the search radius can only be 3 arcseconds.\n",
    "    DR = \"dr2\" # data release\n",
    "    url = f'https://catalogs.mast.stsci.edu/api/v0.1/panstarrs/{DR}/mean/crossmatch/upload?radius=0.0002'\n",
    "    if match_length > single_match_num and len(split_idx) > 2:\n",
    "        print(\"(PS1 crossmatch:Number of matches is too large,%i,Split it automatically\" %single_match_num)\n",
    "    results = []  \n",
    "    uploadCSV_file = os.path.join(uploadCSV_path, \"upload.csv\")\n",
    "    for i in range(len(split_idx)-1):\n",
    "        idx_star = split_idx[i]\n",
    "        idx_end = split_idx[i+1]\n",
    "        match_data[idx_star:idx_end].to_csv(uploadCSV_file, index=True, index_label=\"source_idx\")\n",
    "        r = requests.post(url, files=dict(file=open(uploadCSV_file,'rb')))\n",
    "        PS1_result = pd.read_csv(StringIO(r.text))\n",
    "        try:\n",
    "            PS1_result[\"_searchID_\"] = PS1_result[\"_searchID_\"] + i*(single_match_num) - 2\n",
    "        except:\n",
    "            print(PS1_result)\n",
    "        results.append(PS1_result) \n",
    "    results = pd.concat(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use API to crossmatch GW events with PS1\n",
    "\n",
    "agns = pd.read_csv(f'/data/GW_events/{area}/{O4ab}/{event}_{area}_mq_matched_dec_gtm30.csv')\n",
    "print(len(agns))\n",
    "ra = agns.iloc[:]['RAJ2000']\n",
    "dec = agns.iloc[:]['DEJ2000']\n",
    "match_results = PS1_API_crossmatch(ra, dec,f'/data/GW_events/{area}/{O4ab}/')\n",
    "match_results.to_csv(f'/data/GW_events/{area}/{O4ab}/{event}_{area}_PS1_match_raw.csv',index=False)\n",
    "# match_results = pd.read_csv(f'/data/GW_events/{area}/{O4ab}/{event}_{area}_PS1_match_raw.csv')\n",
    "print(len(match_results))\n",
    "\n",
    "# filter the results by nDetections, gMeanKronMag, rMeanKronMag\n",
    "match_results = match_results.iloc[np.where(match_results['nDetections']>=5)]\n",
    "match_results = match_results.iloc[np.where(np.logical_and(match_results['gMeanKronMag']<21,match_results['rMeanKronMag']<21))]\n",
    "match_results = match_results.iloc[np.where(np.logical_and(match_results['gMeanKronMag']>0,match_results['rMeanKronMag']>0))]\n",
    "print(len(match_results))\n",
    "match_agns = pd.DataFrame(columns=['_ra_','_dec_'])\n",
    "\n",
    "# ulitilize the dstArcSec to select the closest match\n",
    "for i in range(len(agns)):\n",
    "    idx = np.where(np.logical_and(match_results['_ra_']==agns.iloc[i]['RAJ2000'],match_results['_dec_']==agns.iloc[i]['DEJ2000']))\n",
    "    if len(idx)==0:\n",
    "        continue\n",
    "    match_agn = match_results.iloc[idx]\n",
    "    match_agn = match_agn.iloc[np.where(match_agn['dstArcSec']==np.min(match_agn['dstArcSec']))]\n",
    "    if i == 0:\n",
    "        match_agns = match_agn\n",
    "    else:\n",
    "        match_agns = pd.concat([match_agns,match_agn])\n",
    "\n",
    "# JOIN query results with agns table and save the final results\n",
    "match_agns = match_agns.iloc[:,[0,1,42,55,71]]\n",
    "match_agns.columns = ['RAJ2000','DEJ2000','nDetections','gMeanKronMag','rMeanKronMag']\n",
    "results = pd.merge(agns,match_agns,on=['RAJ2000','DEJ2000'],how='right')\n",
    "print(len(results))\n",
    "results.drop_duplicates(subset=['Name'],inplace=True)\n",
    "results.to_csv(f'/data/GW_events/{area}/{O4ab}/{event}_{area}_PS1_match.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
